{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02b68fd2",
   "metadata": {},
   "source": [
    "# CSC3022F: Reinforcement Learning\n",
    "## Lab 1 \n",
    "This lab prac covers key topics of week 1 of the RL component of CSC3022F, which include: \n",
    "* Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836cf16",
   "metadata": {},
   "source": [
    "### To run this notebook:\n",
    "Packages which you will need to ensure that are installed:\n",
    "* numpy\n",
    "Ensure that ```gridworld.py``` is in the same directory as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b5b745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do NOT change this cell. Execute, but leave as is. \n",
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "from gridworld import GridworldEnv # ensure that gridworld.py is in the same directory as this notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e38f3f",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "$$\n",
    "\\def\\E{\\mathbb{E}}\n",
    "\\def\\given{\\mid}\n",
    "\\def\\states{\\mathcal{S}}\n",
    "\\def\\argmax{\\text{argmax}}\n",
    "$$\n",
    "The state-value function $v_\\pi$ for an arbitraty policy (see Sutton & Barto 4.1 for details). The state-value according to a policy is computed as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_\\pi (s) &= \\E_\\pi \\left[ G_t \\given S_t = s\\right]\\\\\n",
    "&= \\E_\\pi \\left[ R_{t+1} + \\gamma G_{t+1} \\given S_t = s\\right]\\\\\n",
    "&= \\E_\\pi \\left[ R_{t+1} + \\gamma v_\\pi ( S_{t+1}) \\mid S_t = s \\right]\\\\\n",
    "&= \\sum_a \\pi(a|s) \\sum_{s^\\prime, r} p(s^\\prime, r \\mid s,a) \\left[r + \\gamma v_\\pi (s^\\prime)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The value of $v_\\pi$ can be updated iteratively for all $s$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{k+1}(s) &= \\E_{\\pi} \\left[ R_{t+1} + \\gamma v_k (S_{t_1} \\given S_t = s)\\right] \\\\\n",
    "&= \\sum_a \\pi(a|s) \\sum_{s^\\prime, r} p(s^\\prime, r \\mid s,a) \\left[r + \\gamma v_\\pi (s^\\prime)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Recall $p$ represents the transition probabilities, which we will get from a simple GridWorld below, $\\gamma$ is the discount factor, whereas $\\theta$ is a  small threshold which indicates when to stop updating our value function. A simple GridWorld looks something like the following:\n",
    "\n",
    "![gridworld](https://i.ibb.co/Lk166s4/gridworld.png)\n",
    "\n",
    "(source: S&B Section 4.1, page 76)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a361bcd9",
   "metadata": {},
   "source": [
    "Implement value iteration based on the following pseudocode:\n",
    "\n",
    "\n",
    "![gridworld.png](https://i.ibb.co/SVTmBFQ/value-iteration.png)\n",
    "\n",
    "(source: S&B Section 4.4, page 83)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b49507e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start state is 4 \n",
      "\n",
      "T  o  o  o\n",
      "x  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n"
     ]
    }
   ],
   "source": [
    "# Do NOT Change this cell. Execute it, but leave as is.\n",
    "env = GridworldEnv(shape=[4, 4], terminal_states=[0,15], terminal_reward=0, step_reward=-1)\n",
    "state = env.reset()\n",
    "print('Start state is', state,'\\n')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99b1d14",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Complete the `value_iteration` function below, based on the above pseudocode.\n",
    "```\n",
    "v = value_iteration(env)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0536ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.observation_space.n is a number of states in the environment. \n",
    "            env.action_space.n is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        V - the optimal value function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.observation_space.n\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.action_space.n containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.action_space.n)\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        \n",
    "        return A\n",
    "\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(env.observation_space.n):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = ...\n",
    "            best_action_value = ...\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = ...\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = ...   \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "      \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2c015f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m v \u001b[38;5;241m=\u001b[39m value_iteration(env)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue Function:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(v)\n",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m, in \u001b[0;36mvalue_iteration\u001b[0;34m(env, theta, discount_factor)\u001b[0m\n\u001b[1;32m     44\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Update the value function. Ref: Sutton book eq. 4.10. \u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     V[s] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m   \n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Check if we can stop \u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m delta \u001b[38;5;241m<\u001b[39m theta:\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'ellipsis'"
     ]
    }
   ],
   "source": [
    "v = value_iteration(env)\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "167764b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test the value function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m expected_v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([ \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,  \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_array_almost_equal(v, expected_v, decimal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour algorithm was successfully implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'v' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the value function\n",
    "expected_v = np.array([ 0, -1, -2, -3, -1, -2, -3, -2, -2, -3, -2, -1, -3, -2, -1,  0])\n",
    "result = np.testing.assert_array_almost_equal(v, expected_v, decimal=2)\n",
    "if result is None:\n",
    "    print(\"Your algorithm was successfully implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572cd08e",
   "metadata": {},
   "source": [
    "#### References\n",
    "This notebook and exercises are based on and adapted from:\n",
    "* Sutton RS, Barto AG. Reinforcement learning: An introduction. MIT press; 2018 Nov 13. https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\n",
    "* Gandhi S. Reinforcement Learning Series. \n",
    "* Micheel M T, Machine Learning. https://www.cs.cmu.edu/~tom/mlbook.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c99a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
